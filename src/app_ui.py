import streamlit as st
import pandas as pd
import os
import zipfile
import io
import tempfile
import hashlib
import re
import logging
from PIL import Image
from datetime import datetime
from data_cleaner import DataCleaner

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å®‰å…¨é…ç½®
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
MAX_FILES = 50
MAX_IMAGE_SIZE = 5 * 1024 * 1024  # 5MB per image
ALLOWED_EXCEL_EXTENSIONS = ['.xlsx', '.xls']
ALLOWED_IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg']

# Page Config
st.set_page_config(
    page_title="Automated Info Pipeline",
    page_icon="ğŸš€",
    layout="wide"
)


# å·¥å…·å‡½æ•°
def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œé˜²æ­¢è·¯å¾„éå†"""
    filename = os.path.basename(filename)
    filename = re.sub(r'[^\w\s.-]', '', filename)
    filename = filename[:100]
    return filename if filename else "unnamed"


def validate_file_magic(file_obj, expected_magic):
    """éªŒè¯æ–‡ä»¶é­”æ•°"""
    file_obj.seek(0)
    magic = file_obj.read(4)
    file_obj.seek(0)
    return magic[:2] == expected_magic


def save_uploaded_file_secure(uploaded_file, extension):
    """å®‰å…¨åœ°ä¿å­˜ä¸Šä¼ æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()[:8]
    safe_filename = f"temp_{timestamp}_{file_hash}{extension}"

    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, safe_filename)

    with open(temp_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    return temp_path


# Title
st.title("ğŸš€ Automated Info-Processing Pipeline")
st.markdown("""
**Automate your tedious operations tasks.**  
This app demonstrates Python's ability to clean data and process images in bulk.
""")

# Sidebar
st.sidebar.header("Configuration")

# ä½¿ç”¨secretsæˆ–ç¯å¢ƒå˜é‡
api_key = st.secrets.get("GEMINI_API_KEY") if hasattr(st, 'secrets') else None

if not api_key:
    api_key = st.sidebar.text_input(
        "Gemini API Key",
        type="password",
        placeholder="Enter your Google Gemini API Key",
        help="âš ï¸ ä¸ä¼šä¿å­˜ï¼Œä»…æœ¬æ¬¡ä¼šè¯ä½¿ç”¨"
    )

if not api_key:
    st.sidebar.warning("âš ï¸ Please provide an API Key to enable AI summarization.")

# Tabs
tab1, tab2 = st.tabs(["ğŸ“Š Intelligent Data Cleaner", "ğŸ–¼ï¸ Batch Image Processor"])

# --- TAB 1: DATA CLEANER ---
with tab1:
    st.header("Excel Data Cleaner")
    st.markdown(
        "Upload a raw 'dirty' Excel file to automatically clean formatting, remove duplicates, and generate an AI summary.")

    uploaded_file = st.file_uploader(
        "Upload Excel File",
        type=['xlsx', 'xls'],
        help=f"æœ€å¤§æ–‡ä»¶å¤§å°: {MAX_FILE_SIZE / 1024 / 1024}MB"
    )

    if uploaded_file:
        try:
            # éªŒè¯æ–‡ä»¶å¤§å°
            if uploaded_file.size > MAX_FILE_SIZE:
                st.error(f"âŒ æ–‡ä»¶å¤§å°è¶…è¿‡ {MAX_FILE_SIZE / 1024 / 1024}MB é™åˆ¶")
                st.stop()

            # éªŒè¯æ–‡ä»¶æ‰©å±•å
            file_ext = os.path.splitext(uploaded_file.name)[1].lower()
            if file_ext not in ALLOWED_EXCEL_EXTENSIONS:
                st.error("âŒ åªæ”¯æŒ Excel æ–‡ä»¶ (.xlsx, .xls)")
                st.stop()

            # éªŒè¯æ–‡ä»¶é­”æ•°
            if not validate_file_magic(uploaded_file, b'PK'):
                st.error("âŒ æ–‡ä»¶æ ¼å¼æ— æ•ˆ")
                st.stop()

            # å®‰å…¨ä¿å­˜æ–‡ä»¶
            temp_path = save_uploaded_file_secure(uploaded_file, file_ext)

            # Show Raw Data
            st.subheader("Raw Data Preview")
            df_raw = pd.read_excel(temp_path, nrows=100)  # é™åˆ¶é¢„è§ˆè¡Œæ•°
            st.dataframe(df_raw.head(10))

            if st.button("ğŸš€ Start Cleaning Pipeline"):
                with st.spinner("Cleaning data and generating AI insights..."):
                    try:
                        # Initialize Cleaner
                        cleaner = DataCleaner(api_key=api_key if api_key else None)

                        # Run Cleaning
                        df_cleaned, summary = cleaner.clean_excel(temp_path)

                        # Display Results
                        st.success("âœ… Cleaning Complete!")

                        col1, col2 = st.columns(2)

                        with col1:
                            st.subheader("Cleaned Data")
                            st.dataframe(df_cleaned.head(10))
                            st.metric("Rows Removed", len(df_raw) - len(df_cleaned))

                        with col2:
                            st.subheader("ğŸ¤– AI Analysis Report")
                            if api_key and summary:
                                st.info(summary)
                            else:
                                st.warning("AI Summary skipped (No API Key provided).")

                        # Download Button
                        output = io.BytesIO()
                        with pd.ExcelWriter(output, engine='openpyxl') as writer:
                            df_cleaned.to_excel(writer, index=False)

                        st.download_button(
                            label="ğŸ’¾ Download Cleaned Excel",
                            data=output.getvalue(),
                            file_name="cleaned_data.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )

                    except pd.errors.ParserError:
                        st.error("âŒ Excel æ–‡ä»¶æ ¼å¼é”™è¯¯")
                        logger.error("Parser error processing Excel file")
                    except Exception as e:
                        st.error("âŒ å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
                        logger.exception(f"Error cleaning Excel: {e}")
                    finally:
                        # æ¸…ç†å†…å­˜
                        del df_raw
                        if 'df_cleaned' in locals():
                            del df_cleaned
                        import gc

                        gc.collect()

            # Cleanup temp file
            if os.path.exists(temp_path):
                os.remove(temp_path)
                logger.info(f"Cleaned up temp file: {temp_path}")

        except Exception as e:
            st.error("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
            logger.exception(f"Error in data cleaner tab: {e}")

# --- TAB 2: IMAGE PROCESSOR ---
with tab2:
    st.header("Batch Image Processor")
    st.markdown(f"""
    Upload multiple images (PNG/JPG) to batch resize and convert them to standard JPEG format.

    **é™åˆ¶ï¼š**
    - æœ€å¤š {MAX_FILES} ä¸ªæ–‡ä»¶
    - æ¯ä¸ªæ–‡ä»¶ä¸è¶…è¿‡ {MAX_IMAGE_SIZE / 1024 / 1024}MB
    """)

    uploaded_images = st.file_uploader(
        "Upload Images",
        type=['png', 'jpg', 'jpeg'],
        accept_multiple_files=True
    )

    if uploaded_images:
        # éªŒè¯æ•°é‡
        if len(uploaded_images) > MAX_FILES:
            st.error(f"âŒ æœ€å¤šåªèƒ½ä¸Šä¼  {MAX_FILES} ä¸ªæ–‡ä»¶")
            st.stop()

        # éªŒè¯å¤§å°
        for img in uploaded_images:
            if img.size > MAX_IMAGE_SIZE:
                st.error(f"âŒ æ–‡ä»¶ {img.name} è¶…è¿‡ {MAX_IMAGE_SIZE / 1024 / 1024}MB é™åˆ¶")
                st.stop()

        if st.button("âš™ï¸ Process Images"):
            with st.spinner(f"Processing {len(uploaded_images)} images..."):
                zip_buffer = io.BytesIO()
                processed_count = 0
                errors = []

                with zipfile.ZipFile(zip_buffer, "w") as zf:
                    for img_file in uploaded_images:
                        try:
                            # æ‰“å¼€å›¾ç‰‡
                            image = Image.open(img_file)

                            # éªŒè¯å›¾ç‰‡
                            if image.size[0] * image.size[1] > 100000000:  # 100MP
                                errors.append(f"{img_file.name}: å›¾ç‰‡å¤ªå¤§")
                                continue

                            # Convert to RGB
                            if image.mode in ('RGBA', 'P', 'L'):
                                image = image.convert('RGB')

                            # Resize
                            image.thumbnail((600, 800), Image.Resampling.LANCZOS)

                            # Save to Bytes
                            img_byte_arr = io.BytesIO()
                            image.save(img_byte_arr, format='JPEG', quality=85, optimize=True)

                            # æ¸…ç†æ–‡ä»¶å
                            safe_name = sanitize_filename(img_file.name)
                            new_filename = os.path.splitext(safe_name)[0] + ".jpg"

                            # Add to Zip
                            zf.writestr(new_filename, img_byte_arr.getvalue())

                            processed_count += 1

                            # æ¸…ç†å†…å­˜
                            image.close()

                        except Exception as e:
                            errors.append(f"{img_file.name}: å¤„ç†å¤±è´¥")
                            logger.error(f"Error processing {img_file.name}: {e}")

                # æ˜¾ç¤ºç»“æœ
                if processed_count > 0:
                    st.success(f"ğŸ‰ æˆåŠŸå¤„ç† {processed_count} ä¸ªå›¾ç‰‡!")

                    if errors:
                        st.warning(f"âš ï¸ {len(errors)} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
                        with st.expander("æŸ¥çœ‹é”™è¯¯"):
                            for error in errors:
                                st.text(error)

                    # Download Zip
                    st.download_button(
                        label="ğŸ“¦ Download All (ZIP)",
                        data=zip_buffer.getvalue(),
                        file_name="processed_images.zip",
                        mime="application/zip"
                    )
                else:
                    st.error("âŒ æ‰€æœ‰å›¾ç‰‡å¤„ç†å¤±è´¥")